{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "700d96bc",
   "metadata": {},
   "source": [
    "# Data preparation\n",
    "Data - Information - Knowledge\n",
    "While *data* is, what is stored and transported (for example strings, integers, pixels, etc.), *information* is a result of data interpretation when analysing and visualizing. But on top of data and information comes *knowledge* with comparing, remembering and learning.  \n",
    "\n",
    "In the first part of this notebook we are going to take a look at data preparation using python, covering the criteria for data quality, data cleaning and data summarising, combining and reducing.\n",
    "Even though an incredible amount of data is generated and used every day, high quality and up-to-date datasets are rare. Furthermore, pushing data around (converting to other formats, or data structure, etc) could rise in data loss, therefore always check the data before moving on to the next step. \n",
    "\n",
    "**Important**: *Just a little hint, all data we use in this course are focusing on Aachen. Since this is a German city, the data we are using is mainly in German. We do try to translate as often as possible and most of the processing is focussing on geolocated or statistical data however, it could be that there are German words coming up.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14f2129-32b5-4cb6-b894-9a0dfcf62b87",
   "metadata": {},
   "source": [
    "## Data quality\n",
    "Let us start with the criteria of data quality. These criteria are:\n",
    "\n",
    "* Correctness/ Accuracy\n",
    "* Reliability/ Credibility\n",
    "* Consistency\n",
    "* Completeness\n",
    "* Comprehensibility\n",
    "* No Redundancy\n",
    "* Timeliness\n",
    "* Accessibility "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac85cc0-339e-4d04-b891-a900fbf65a54",
   "metadata": {},
   "source": [
    "So far so good, well having these criteria is one thing, applying them on a data set another. So let us take the example **Recyclinghöfe.json** (which is actually a geoJSON, as you will see) and check if it matches the criteria. \n",
    "\n",
    "When we go through the list, we will see, that the data file **Recyclinghöfe.json** was downloaded from the Aachen [Open data Portal](https://offenedaten.aachen.de/). \n",
    "\n",
    "**Reliability/ Credibility** - \n",
    "They do provide a few more information about this data set such as: *Source, Author, Person responsible, Last updated, Created*. However, it is not provided any information on how the data was created. For its purpose to be an example of data within this course, it is enough. However, depending on the purpose the data is meant to full fill, more information are maybe required.   \n",
    "\n",
    "**No redundancy** - \n",
    "Well, if one check the plot of our read data file one may discover that there is some data occurring twice. You may have seen it already, it is the longitude and latitude information. This information is once in the geometry column as point definition and also in separated columns *lat* and *log*. This does not necessarily mean it is redundant since the differentiation in log and lat allows to know what is what, since within the geometry column this is actually not defined (common sense may tell you, the other option is not even close to Germany, but well sometimes no one cares about common sense). Anyway also we could talk about a redundancy here, it is not a troubling issue which needs to be solved for our purpose.\n",
    "\n",
    "![redundancy](./img/redundancy.png)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3068b6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fea998",
   "metadata": {},
   "source": [
    "### Reading/ Parsing all necessary files\n",
    "Let us start by reading the data file **Recyclinghöfe.json** using the Python package GeoPandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9e41af-6a1c-40ec-a61d-37dc17b0d9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "recycling = gpd.read_file(\"data/Recyclinghöfe.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e41cd37",
   "metadata": {},
   "source": [
    "GeoPandas now has read the data from the geoJSON file with its geometric information and the data on the attributes and parsed them. The result is a table, a **GeoDataFrame**. We can inspect the table using the **.plot()** function offered by the GeoPandas GeoDataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d03f753",
   "metadata": {},
   "outputs": [],
   "source": [
    "recycling.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0f790a",
   "metadata": {},
   "outputs": [],
   "source": [
    "recycling.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2909022d",
   "metadata": {},
   "source": [
    "## Data cleaning\n",
    "We have a few options to clean our data set:\n",
    "\n",
    "* Ignore data set\n",
    "* Manual insertion\n",
    "* Global constant\n",
    "* Location parameter\n",
    "* Restricted location parameter\n",
    "* Most probable value\n",
    "* Time series and spatial data\n",
    "\n",
    "Since only the fact, that we have two global constants, occurs in the **Recyclinghöfe.json** file, this is the only interesting aspect. However, the global constants used in our example file is not disturbing and actually does not require any cleaning since it has no direct impact on our purpose. But in case we would want to know, in which district of Aachen each of this recycling facilities is placed, then this information would be helpful.\n",
    "\n",
    "Well, since we have the location given, and the data set only consists of 4 objects, we could go ahead and figure out in which district of Aachen each of this recycling facilities is placed. This is a manual task to do. One would search for the **ortsteil** by searching for the address and then manually enter the result in the geoJSON file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19a0b36-0ecf-41a0-a057-71ccd38dab24",
   "metadata": {},
   "source": [
    "## Data - Combining, summarizing, reducing \n",
    "We not go to cover all three, but focus on an example for combining data. \n",
    "Therefore, we need another example apart from **Recyclinghöfe.json** since this file does not enough data, or let us say interesting enough data to be an useful example. Instead of Recyclinghöfe.json we choose two other data files which are **einwohnerstatistik-31.12.2020.csv** and **StatistischeBezirkeAachen.shp**. \n",
    "So let us start with (again) reading the data files using Python GeoPandas. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94290363-542c-46a7-b425-b5c7746715b5",
   "metadata": {},
   "source": [
    "**Reading StatistischeBezirkeAachen.shp**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a3e136-a184-4c8d-b059-40c22d5340c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "districts = gpd.read_file(\"./data/aachen/StatistischeBezirkeAachen.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5319d9e4-ce24-41ce-b5bf-fe829723063d",
   "metadata": {},
   "outputs": [],
   "source": [
    "districts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e9ca6e-d2a3-434b-81bd-e5d4823185ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "districts.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a75f40-545a-4b60-9723-4fd37ee03f67",
   "metadata": {},
   "source": [
    "**Reading einwohnerstatistik-31.12.2020.csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf41fd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics = pd.read_csv(\"./data/aachen/einwohnerstatistik-31.12.2020.csv\")\n",
    "statistics.head()\n",
    "statistics.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344267e5",
   "metadata": {},
   "source": [
    "### Investigating the information for commonalities\n",
    "After reading the files using GeoPandas we can start checking them for information we need. But first, we require to clarify what we are going to do. Well we want to combine the two data set and in order to do this we require information based on which we can perform a combination - so let us check both files for data that allows us to combine them.\n",
    "The CSV file is a plain table file, which means it does not include any geometric information. However both data file have one feature in common: the **id** of the area. Even though the columns names differ, they are the same entity though. Let's order the tables by the respective id and see whether they match.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e6f72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resorting the statistic DataFrame\n",
    "sorted_stats = statistics.sort_values(by='StatBezName')\n",
    "sorted_stats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae9ffa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resorting the districts GeoDataFrame\n",
    "sorted_districts = districts.sort_values(by='ST_NAME')\n",
    "sorted_districts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc142294",
   "metadata": {},
   "source": [
    "Since we know now, that **id** is an attribute both tables actually have even though it is named differently. \n",
    "With this information we can add the information from the extended statistics DataFrame we have assigned in `statistics` to the GeoDataFrame. Here, every column of the `statistics` data set is added to the GeoDataFrame table. With tabular data, this concept is referred to as **merging** or **joining** data. \n",
    "\n",
    "An explanation on merging GeoDataFrames is provided here: https://geopandas.org/docs/user_guide/mergingdata.html "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447f3fbc-c859-459e-aa60-22d1335b3831",
   "metadata": {},
   "source": [
    "In order to perform a **join** we need to make sure to define on WHAT we are going to join and how. In our case, joining the *statistics* DataFrame to the *districts* GeoDataFrame from the left.\n",
    "```\n",
    "bezirke <-- statistics\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0714e564-8a97-49bc-bca6-dc126646ae33",
   "metadata": {},
   "source": [
    "### Renaming columns \n",
    "First however, we have to rename the column name *Bez* from the *statistics* DataFrame to match the respective name *STATBEZ* column in our *districts* GeoDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8307eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics = statistics.rename(columns = {'Bez' : 'STATBEZ'})\n",
    "statistics.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e467f18",
   "metadata": {},
   "source": [
    "### Perparing information\n",
    "In a next step, we clean our data. As already talked about earlier in this notebook, clean data is essential for efficient and qualitative data processing, and therefore we need to get rid of unwanted elements such as *NaN*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad123959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.to_numeric(statistics[\"STATBEZ\"])\n",
    "import numpy as np\n",
    "statistics = statistics.replace(r'^\\s*$', np.nan, regex=True)\n",
    "index = statistics[statistics['STATBEZ'] == np.nan].index\n",
    "#statistics.drop(34, inplace=True)\n",
    "statistics.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b868f2",
   "metadata": {},
   "source": [
    "Next we have an example for using NumPy dtype. Here, we define the column content for *STATBEZ* as type integer64. Integer, as we want them to be hole numbers not floating point numbers and 64 defines the size of date, here 64 bytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c00aa3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics[\"STATBEZ\"] = statistics[\"STATBEZ\"].fillna(0)\n",
    "statistics[\"STATBEZ\"] = statistics[\"STATBEZ\"].astype('int64')\n",
    "\n",
    "statistics[\"STATBEZ\"].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09d85fd",
   "metadata": {},
   "source": [
    "### Merging information into one DataFrame\n",
    "Finally, we get to the point where we can actually merge our two data sets. \n",
    "We do this by using the Python GeoPandas *merge* method (remember, a function is called a method if it is defined within a class).  \n",
    "\n",
    "You may have seen the *tail* method, this in contrast to the *head* method gives us the last rows of a DataFrame table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abea137a",
   "metadata": {},
   "outputs": [],
   "source": [
    "districts_stats = districts.merge(statistics, on='STATBEZ', how='left')\n",
    "# districts_stats = districts.set_index('STATBEZ').join(statistics.set_index('STATBEZ'))\n",
    "districts_stats.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73de88c3-0040-45fe-b17c-e0520aebe46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "districts_stats = gpd.GeoDataFrame(districts_stats)\n",
    "# type is used to get to know what python type or data type in python a specific entity is. Here for example we get to know that the variable districts_stats is a GeoDataFrame\n",
    "type(districts_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5e9cf1-c013-4e94-8834-9460808aaf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "districts_stats.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c34380c-ba65-40dd-a146-edc34a760432",
   "metadata": {},
   "source": [
    "### Adding information\n",
    "Next, we are going to create new information by combining existing ones. To be more precise, we are going to add the **population density**. We can do this because we already know the districts since we get their geometry as a polygon (check the GeoDataFrame geometry column) and since we just merged these two data sets we also have the number of people living in each district.\n",
    "\n",
    "But first we need some visualization because this is just easier for humans to read. Looking at a nice pretty picture is so much more fun than text... or code. Therefore, let us create a plot visualization using Python matplotlib library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3f5035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set to min and max of data\n",
    "vmin, vmax = 0, 133623\n",
    "\n",
    "# create figure and axes for Matplotlib\n",
    "fig, ax = plt.subplots(1, figsize=(14,6))\n",
    "# add a title and annotation\n",
    "ax.set_title('Total population of Aachen', fontdict={'fontsize': '15', 'fontweight' : '3'})\n",
    "\n",
    "ax = districts_stats.plot(column='Pers', cmap = 'YlGnBu', ax=ax, legend = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b3bd25-54c4-4d49-8aba-840e72c0a959",
   "metadata": {},
   "source": [
    "Now let us get the area size of each district. \n",
    "\n",
    "First, we slice the GeoDataFrame to get the geometry column content only. Now we have the polygons (represented by points) and we now want to get the area. Fortunately we do not need to calculate it ourselves, but Python GeoPanda provides us with the *area* method, and this it what we are going to use here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee1c842",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(districts_stats[1:5][\"geometry\"].area)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6107fc-2754-4b0d-a91b-8e81f58719ec",
   "metadata": {},
   "source": [
    "Well, the area calculation seems to be successful, so we can move on and put all components in the formula to calculate the population density (here called *popdens*).\n",
    "\n",
    "Additionally, since we not only want to calculate the population density, but we also want to save this information, we are going to add a new column to our GeoDataFrame by adding a *popdens* column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7787f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "districts_stats['popdens'] = districts_stats[\"Pers\"] / districts_stats[\"geometry\"].area*1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11a3af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "districts_stats.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beefbadf-e190-43fe-bab9-8bca75ced50f",
   "metadata": {},
   "source": [
    "In a last and final step, we want to visualize the results again. Since visualization is one of the most important aspects in data processing, always try to find a way to meaningfully represent your data and results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55db88fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set to min and max of data\n",
    "vmin, vmax = 0, 133623\n",
    "\n",
    "# create figure and axes for Matplotlib\n",
    "fig, ax = plt.subplots(1, figsize=(14,6))\n",
    "# add a title and annotation\n",
    "ax.set_title('Population Density of Aachen in 1000 inhabitants per km^2', fontdict={'fontsize': '15', 'fontweight' : '3'})\n",
    "\n",
    "ax = districts_stats.plot(column='popdens', cmap = 'YlGnBu', ax=ax, legend = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
