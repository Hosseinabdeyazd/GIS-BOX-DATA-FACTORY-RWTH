{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data analysis\n",
    "Having a set of clean qualitative data is only one step, another step is analysing this data according to gain information or even knowledge. Therefore, we are going to take a look into **Pattern Analysis** and **Cluster Analysis**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Point Pattern Analysis\n",
    "To be more precise, we focus on point pattern analysis.\n",
    "\n",
    "A **pattern analysis** involves:\n",
    "* Identification\n",
    "* Quantification\n",
    "* Visualization\n",
    "\n",
    "It is important for identifying how geometric phenomena behave, and therefore can be seen as a comparison of pattern through statistical quantification.  \n",
    "One can say it answers the question: *Is there a spatial clustering?*  \n",
    "\n",
    "A **point pattern** can be seen as a collection of events, which take place in several locations. Analysing this spatial arrangements of points in a usually 2-dimensional space is called *point pattern analysis*. Therefore, the *space* is a predefined study region.  \n",
    "A set of points, let us call them $X$, with $X = {x \\in D}. D$ is therefore the study region and a subset of the $R^n$ (n-dimensional Euclidean space).  \n",
    "\n",
    "But before being too theoretical, let us work this out by using an example:\n",
    "### Point pattern file import\n",
    "To provide a meaningful example, we will not use the example **Recyclingh√∂fe.json** but instead use **Haltestellen.csv**. The new example provides information about bus stops in Aachen and there are a lot of bus stops in Aachen. Our data source is the AVV. They provide information of their bus system. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation for point pattern analysis\n",
    "Let us get started: First, we need to read the CSV file. As the table shows, there are just little information, however, imported for us are the coordinates' information. \n",
    "\n",
    "On first glimpse the coordinates' representation looks a little different, however, having a look at the README file provided by the AVV, one knows why the coordinates look like this, they are multiplied by 1000000. So, before using them, we need to divide all coordinate content by 1000000 to get the coordinate representation our pandas and GeoPandas methods and functions can work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas as pd\n",
    "import geopandas\n",
    "import pysal\n",
    "import seaborn\n",
    "import contextily\n",
    "import os\n",
    "import matplotlib.pyplot as plt \n",
    "avv_halt = pd.read_csv(r\"./data/AVV_Haltestellen.csv\", sep=',')\n",
    "avv_halt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us get all existing coordinate elements and divide the first column *WGS84_RW_X_1000000* than we do the second one *WGS84_HW_X_1000000*.  \n",
    "To make it easier, one could implement a function instead, to keep it clean and simple. But in this example, we use the longer way to show you what will happen.\n",
    "\n",
    "First, we Slice the DataFrame focusing on the *WGS84_RW_X_1000000* column. Next we create two empty lists called old and new. With this preparation we can iterate over the column *WGS84_RW_X_1000000*. We first append all elements to the list called old before we go ahead and divide the element by 1000000 and append the result to the list called new."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First changing data from the first column\n",
    "ort = avv_halt['WGS84_RW_X_1000000'] \n",
    "old = []\n",
    "new = []\n",
    "for elem in ort:\n",
    "    old.append(elem)\n",
    "    elem = elem / 1000000\n",
    "    new.append(elem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a next step we are going to replace all old coordinate representations (we still have them in our list called old) with the new coordinate representation within the DataFrame.\n",
    "\n",
    "If you compare both DataFrames, the one from above and the one we are just about to plot, you will see a change in column *WGS84_RW_X_1000000*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avv_halt['WGS84_RW_X_1000000'].replace(old, new, inplace = True)\n",
    "avv_halt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same procedure happens for the other column *WGS84_HW_X_1000000* (at this point it would be wise to create for example a function to avoid duplication in the code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Second changing data from the second column\n",
    "ort2 = avv_halt['WGS84_HW_X_1000000'] \n",
    "old2 = []\n",
    "new2 = []\n",
    "for elem2 in ort2:\n",
    "    old2.append(elem2)\n",
    "    elem2 = elem2 / 1000000\n",
    "    new2.append(elem2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avv_halt['WGS84_HW_X_1000000'].replace(old2, new2, inplace = True)\n",
    "avv_halt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bit column names *WGS84_RW_X_1000000* and *WGS84_HW_X_1000000* are quite long, so in a next step, we change both, one to longitude and the other one to latitude, as they are not just shorter but also more intuitive.\n",
    "\n",
    "**Hint**: Intuitive wording can be quite helpful when it comes to naming something, for example variables but also columns etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avv_halt = avv_halt.rename(columns = {'WGS84_HW_X_1000000' : 'latitude'})\n",
    "avv_halt = avv_halt.rename(columns = {'WGS84_RW_X_1000000' : 'longitude'})\n",
    "avv_halt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avv_halt.to_csv('avv_stops.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting graphs\n",
    "We want to know if there actually is spatial clustering. To do this, we could first go ahead and plot out points in a dependent graph, just to see if there are any conspicuous features. And you can probably see for yourself, there are!\n",
    "So, we use Seaborn here to plot our points in a dependent graph, where the points are ordered and the point dataset in general gets a dimension. \n",
    "\n",
    "The result is: One can see point concentration in different areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.jointplot(x='longitude', y='latitude', data=avv_halt, s=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster analysis\n",
    "So far we only have a clue that there could be a spatial clustering, so we could move on from pattern analysis to cluster analysis. But first let us talk about the difference between a pattern and a cluster. Well, a pattern represents conspicuous features and/or designs. A cluster however, is more location focused, answering the questions: *Where are hot spots and cold spots?*, *Where are incidents most dense?*.\n",
    "\n",
    "Keep this difference in mind when you consider performing pattern and cluster analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hex-binning\n",
    "In a next step, let us make the cluster structures more visible.\n",
    "\n",
    "Binning is a technique for data aggregation, used for grouping data of a dataset. Therefore, the dataset of *n* values will be converted into less than *n* discrete groups. We just consider 2-dimensional (x, y coordinates) datasets. The technique is simple: A plane (x, y) is uniformly tiled in polygons, which can be squares, rectangles or hexagons.\n",
    "Which means: Hex-binning is a specific way for creating choropleth maps. The main advantage of hexagons in comparison with rectangles is, that they are more similar to circles than squares, therefore the data aggregation around a bin centre is more efficient, when similar values provoke a lot of over plotting.\n",
    "\n",
    "Let us take a look at an example:  \n",
    "\n",
    "When many bus stops (points) are concentrated in some area, it can become hard to explore the patterns' nature (see the graph plot from above). To simplify and improve the visibility, one generate a regular grid (hexagonal). Depending on how many points fall in one grid cell, the colour is chosen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, figsize=(12, 9))\n",
    "#Generating the hexbin plot\n",
    "hb = ax.hexbin(\n",
    "    avv_halt['longitude'],\n",
    "    avv_halt['latitude'], \n",
    "    gridsize=50, \n",
    "    linewidths=0,\n",
    "    alpha=0.5, \n",
    "    cmap='viridis_r'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much nicer and much more recognizable for the human eye. Additionally, to the plotted graph, we can add a *basemap* to add a geografic lication.\n",
    "\n",
    "Next, we use the library *contextily*, because one can use the **add_basemap()** method to add a base map to provide a context. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, figsize=(12, 9))\n",
    "#Generating the hexbin plot\n",
    "hb = ax.hexbin(\n",
    "    avv_halt['longitude'],\n",
    "    avv_halt['latitude'], \n",
    "    gridsize=50, \n",
    "    linewidths=0,\n",
    "    alpha=0.5, \n",
    "    cmap='viridis_r'\n",
    ")\n",
    "#Adding a basemap\n",
    "contextily.add_basemap(\n",
    "    ax, \n",
    "    crs=\"EPSG:4326\",\n",
    "    source=contextily.providers.CartoDB.Positron\n",
    ")\n",
    "plt.colorbar(hb)\n",
    "ax.set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel density estimation (KDE)\n",
    "Almost at the end! The final method for cluster representation we want to show you is the **Kernal Density Estimation**. \n",
    "\n",
    "The grids are the spatial equivalent of a histogram. An alternative is the Kernel Density Estimation (an empirical approximation of the probability density function). Instead of a grid of hexagons, the *kde* uses a grid of points is placed over the area of interest, on which it placed kernel functions that count points around them with different weight based on the distance. Furthermore, the counts are aggregated to generate a (global) surface with probabilist. The most common kernel function is the Gaussian one. Here, a normal distribution to weight points in applied. The result is a continuous surface. The following function creates a Gaussian kernel. \n",
    "\n",
    "To implement the method, we use the *kdeplot* method from Python *seaborn* library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, figsize=(9, 9))\n",
    "#Generating the kde plot\n",
    "seaborn.kdeplot(\n",
    "    x = avv_halt['longitude'],\n",
    "    y = avv_halt['latitude'], \n",
    "    n_levels=50, \n",
    "    shade=True,\n",
    "    alpha=0.55, \n",
    "    cmap='viridis_r'\n",
    ")\n",
    "#Adding a basemap\n",
    "contextily.add_basemap(\n",
    "    ax,\n",
    "    crs=\"EPSG:4326\",\n",
    "    source=contextily.providers.CartoDB.Positron,\n",
    "    \n",
    ")\n",
    "ax.set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Look at the point analysis again carefully and note which steps are taken here and why. Describe what happens whereby adding notes '#' in the code. If you are working with code, it is important that you understand the steps. Change the code and see what happens. Load the file 'nodes-forest_ac.geojson' and use the same algorithm but use different layout information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "668.717px",
    "left": "940px",
    "top": "171.133px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
